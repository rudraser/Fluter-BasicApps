{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_ResNet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudraser/Fluter-BasicApps/blob/main/Custom_ResNet_attempt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOcx0lBgghLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0914aaaf-447f-4bdd-dd17-d634d468ed1a"
      },
      "source": [
        "import torch\n",
        "import torchvision # torch package for vision related things\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))\n",
        "print(f\"Training on device {device}.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouBDlC9Jgoqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f67c53c-dd69-404b-9fca-6f6577441df2"
      },
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "data_path = '/data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomCrop(size=[32,32], padding=4),                        \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i03NlEVDgyMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433cd6ce-0fc6-4c0b-d200-bb852faf7d50"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = '/data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomCrop(size=[32,32], padding=4),                        \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWnb-y7QTZM6",
        "outputId": "6fb672ff-1305-4c89-af9c-0c7c3bf3974b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([               #loading the val set\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "792a2ZhoskbM"
      },
      "source": [
        "class resblock(nn.Module):                    #creating a resblock which would be used iterativealy.\n",
        "    def __init__(self, n):\n",
        "        super(resblock, self).__init__()\n",
        "        self.conv = nn.Conv2d(n, n, kernel_size=3,\n",
        "                              padding=1, bias=False)                 # defining layers for forward\n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n)\n",
        "\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
        "                                      nonlinearity='relu')  \n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)                                 #Actual resblock with the skip connection over 1 layer\n",
        "        out = self.batch_norm(out)\n",
        "        out = torch.relu(out)\n",
        "        return out + x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7PIfI9Cg4VD"
      },
      "source": [
        "class deep(nn.Module):                # the model which uses resblock iteratively and some other layers\n",
        "    def __init__(self, n1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n1 = n1\n",
        "        self.conv1 = nn.Conv2d(3, n1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n1, 2*n1, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(2*n1, 4*n1, kernel_size=3, padding=1)       # defing layers for forward\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *(n_blocks * [resblock(n=n1)]))\n",
        "        self.resblocks2 = nn.Sequential(\n",
        "            *((n_blocks*2) * [resblock(n=n1*2)]))\n",
        "        self.resblocks3 = nn.Sequential(\n",
        "            *(n_blocks * [resblock(n=n1*4)]))\n",
        "        self.fc1 = nn.Linear(8 * 8 * n1*4, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):                    # using the resblocks 3 times, each with twice the number of channels as before\n",
        "        out = torch.relu(self.conv1(x))\n",
        "        out = self.resblocks(out)                          \n",
        "        out = F.max_pool2d(out, 2) \n",
        "        out = self.conv2(out)\n",
        "        out = self.resblocks2(out)\n",
        "        out = F.max_pool2d(out, 2) \n",
        "        out = self.conv3(out)\n",
        "        out = self.resblocks3(out)\n",
        "        out = out.view(-1, 8 * 8 * self.n1*4)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVBuJngWmhMe"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):           # defining the training loop\n",
        "    for epoch in range(1, n_epochs + 1):                                        # these inputs would be defined in next cell\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        \n",
        "        print('{} Epoch {}, Training loss {}'.format(\n",
        "            datetime.datetime.now(), epoch,\n",
        "            loss_train / len(train_loader)))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8S3Yvirmh-U"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)                   # defining mini batch size and shuffling\n",
        "\n",
        "model =  deep().to(device=device)# <1>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0AfUtEimwEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabe6239-1120-4d8c-c71b-c4b44fb3f052"
      },
      "source": [
        "\n",
        "training_loop(                   #calling the training loop finally\n",
        "    n_epochs = 60,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 05:00:39.970732 Epoch 1, Training loss 1.7361613687346964\n",
            "2021-07-31 05:01:46.261139 Epoch 2, Training loss 1.3139591197223615\n",
            "2021-07-31 05:02:52.369499 Epoch 3, Training loss 1.0961830253948641\n",
            "2021-07-31 05:03:58.608086 Epoch 4, Training loss 0.9618200964634985\n",
            "2021-07-31 05:05:04.779977 Epoch 5, Training loss 0.878183582669024\n",
            "2021-07-31 05:06:11.018292 Epoch 6, Training loss 0.821953491900888\n",
            "2021-07-31 05:07:17.100934 Epoch 7, Training loss 0.7673381805953468\n",
            "2021-07-31 05:08:23.241040 Epoch 8, Training loss 0.7284360977313707\n",
            "2021-07-31 05:09:29.598740 Epoch 9, Training loss 0.7007892883723349\n",
            "2021-07-31 05:10:35.618412 Epoch 10, Training loss 0.6624118571772295\n",
            "2021-07-31 05:11:41.712772 Epoch 11, Training loss 0.642104980966929\n",
            "2021-07-31 05:12:47.957792 Epoch 12, Training loss 0.6146787089460036\n",
            "2021-07-31 05:13:54.050863 Epoch 13, Training loss 0.5905050756147755\n",
            "2021-07-31 05:15:00.181673 Epoch 14, Training loss 0.5733622479850374\n",
            "2021-07-31 05:16:06.356902 Epoch 15, Training loss 0.5581835471951139\n",
            "2021-07-31 05:17:12.438156 Epoch 16, Training loss 0.540512422466522\n",
            "2021-07-31 05:18:18.549245 Epoch 17, Training loss 0.5256236817716332\n",
            "2021-07-31 05:19:24.702984 Epoch 18, Training loss 0.5152795598520648\n",
            "2021-07-31 05:20:31.113921 Epoch 19, Training loss 0.5007642269744288\n",
            "2021-07-31 05:21:37.116286 Epoch 20, Training loss 0.4918702084885534\n",
            "2021-07-31 05:22:43.348403 Epoch 21, Training loss 0.4781777127967466\n",
            "2021-07-31 05:23:49.540948 Epoch 22, Training loss 0.46822069242329856\n",
            "2021-07-31 05:24:55.585632 Epoch 23, Training loss 0.46183695093445154\n",
            "2021-07-31 05:26:01.249966 Epoch 24, Training loss 0.45969798699821657\n",
            "2021-07-31 05:27:06.835252 Epoch 25, Training loss 0.44283872663669877\n",
            "2021-07-31 05:28:13.347272 Epoch 26, Training loss 0.43841203869989764\n",
            "2021-07-31 05:29:19.449038 Epoch 27, Training loss 0.4299127914182975\n",
            "2021-07-31 05:30:25.730317 Epoch 28, Training loss 0.4252714419265842\n",
            "2021-07-31 05:31:31.952535 Epoch 29, Training loss 0.42268617580766266\n",
            "2021-07-31 05:32:37.898067 Epoch 30, Training loss 0.4102586591068436\n",
            "2021-07-31 05:33:43.922114 Epoch 31, Training loss 0.40742334006997327\n",
            "2021-07-31 05:34:49.890335 Epoch 32, Training loss 0.40570984881780947\n",
            "2021-07-31 05:35:55.770310 Epoch 33, Training loss 0.39755837876549766\n",
            "2021-07-31 05:37:01.753127 Epoch 34, Training loss 0.3909232584221284\n",
            "2021-07-31 05:38:07.694420 Epoch 35, Training loss 0.3877739000427144\n",
            "2021-07-31 05:39:13.554691 Epoch 36, Training loss 0.38465260777174665\n",
            "2021-07-31 05:40:19.453826 Epoch 37, Training loss 0.3792291420709599\n",
            "2021-07-31 05:41:25.533403 Epoch 38, Training loss 0.37400862800381374\n",
            "2021-07-31 05:42:31.281106 Epoch 39, Training loss 0.3742162335444899\n",
            "2021-07-31 05:43:37.035608 Epoch 40, Training loss 0.3656131815822685\n",
            "2021-07-31 05:44:42.892840 Epoch 41, Training loss 0.36644293244003945\n",
            "2021-07-31 05:45:48.597726 Epoch 42, Training loss 0.36684812740672884\n",
            "2021-07-31 05:46:54.486708 Epoch 43, Training loss 0.3623254961331787\n",
            "2021-07-31 05:48:00.346375 Epoch 44, Training loss 0.3575970780895189\n",
            "2021-07-31 05:49:06.067808 Epoch 45, Training loss 0.3576785323526853\n",
            "2021-07-31 05:50:11.701865 Epoch 46, Training loss 0.35537984467985684\n",
            "2021-07-31 05:51:17.456317 Epoch 47, Training loss 0.3496201881934005\n",
            "2021-07-31 05:52:23.457919 Epoch 48, Training loss 0.3462679853658085\n",
            "2021-07-31 05:53:29.275076 Epoch 49, Training loss 0.3441121934358116\n",
            "2021-07-31 05:54:34.871256 Epoch 50, Training loss 0.341782119096545\n",
            "2021-07-31 05:55:40.482973 Epoch 51, Training loss 0.3435641158648464\n",
            "2021-07-31 05:56:46.057232 Epoch 52, Training loss 0.335415470382899\n",
            "2021-07-31 05:57:51.518055 Epoch 53, Training loss 0.33693623657116806\n",
            "2021-07-31 05:58:57.255463 Epoch 54, Training loss 0.3342140545625516\n",
            "2021-07-31 06:00:02.888457 Epoch 55, Training loss 0.3317970135022917\n",
            "2021-07-31 06:01:08.513505 Epoch 56, Training loss 0.3297531934231139\n",
            "2021-07-31 06:02:14.162413 Epoch 57, Training loss 0.3305618576229076\n",
            "2021-07-31 06:03:19.923224 Epoch 58, Training loss 0.3264826439568759\n",
            "2021-07-31 06:04:25.410965 Epoch 59, Training loss 0.32595857646306764\n",
            "2021-07-31 06:05:30.947624 Epoch 60, Training loss 0.32480630068980215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbfjst0E2g5G",
        "outputId": "070d1c1c-8958-4498-9782-984ba7160324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():   # we do no need backpropagation for this hence the nograd\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # taking the class at maximum probability as prediction\n",
        "                total += labels.shape[0]  \n",
        "                correct += int((predicted == labels).sum())   # calculating the no. of correct predictions.\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.87\n",
            "Accuracy val: 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbrdxh876qaJ"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=0.01)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRhQJbKh62kt",
        "outputId": "4a8fb877-ea10-4186-b2b8-163a346165fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_loop(   # 10 iterations with new LR\n",
        "    n_epochs = 10,                      \n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 06:45:21.602253 Epoch 1, Training loss 0.22703731442561081\n",
            "2021-07-31 06:46:26.884151 Epoch 2, Training loss 0.20902931752145443\n",
            "2021-07-31 06:47:32.521110 Epoch 3, Training loss 0.2060614214385943\n",
            "2021-07-31 06:48:37.958922 Epoch 4, Training loss 0.2059209347676362\n",
            "2021-07-31 06:49:43.403156 Epoch 5, Training loss 0.2033590307016202\n",
            "2021-07-31 06:50:48.684747 Epoch 6, Training loss 0.20368947439333973\n",
            "2021-07-31 06:51:53.941052 Epoch 7, Training loss 0.20252378454522404\n",
            "2021-07-31 06:52:59.256680 Epoch 8, Training loss 0.19996077006163498\n",
            "2021-07-31 06:54:04.548188 Epoch 9, Training loss 0.20431800866904465\n",
            "2021-07-31 06:55:09.705806 Epoch 10, Training loss 0.20122921518752795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukeetie07bdu",
        "outputId": "63967b5f-405e-4c7f-f281-8b555f2e813c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "validate(model, train_loader, val_loader)           #now checking acc"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.93\n",
            "Accuracy val: 0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gcu36ec63xZ"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.01)     #further lowering LR"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWe10Nu67XNm",
        "outputId": "8b8f1e29-35a5-4a86-c74d-ce335c8cc6d2"
      },
      "source": [
        "training_loop(                    # another 10 iterations\n",
        "    n_epochs = 10,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 06:57:18.192641 Epoch 1, Training loss 0.12092679471034756\n",
            "2021-07-31 06:58:23.589142 Epoch 2, Training loss 0.09502103105854348\n",
            "2021-07-31 06:59:28.686982 Epoch 3, Training loss 0.08616054943069587\n",
            "2021-07-31 07:00:33.795954 Epoch 4, Training loss 0.08041405860963456\n",
            "2021-07-31 07:01:38.920261 Epoch 5, Training loss 0.07762975376718642\n",
            "2021-07-31 07:02:44.016825 Epoch 6, Training loss 0.06999396136306855\n",
            "2021-07-31 07:03:49.534051 Epoch 7, Training loss 0.06919175327179092\n",
            "2021-07-31 07:04:54.611408 Epoch 8, Training loss 0.06653464321866441\n",
            "2021-07-31 07:05:59.639167 Epoch 9, Training loss 0.06344245725949688\n",
            "2021-07-31 07:07:04.652716 Epoch 10, Training loss 0.06189026420011812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJETy0e-7X0q",
        "outputId": "233d897b-881a-41c3-d3f0-bcb830202b06"
      },
      "source": [
        "validate(model, train_loader, val_loader)           #now checking acc"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.98\n",
            "Accuracy val: 0.88\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}